{
    "collab_server" : "",
    "contents" : "\nbase_line_model_xgboost = function(data_train, train_label, data_test, test_label, target, best_model=FALSE, nrounds = 100, type = \"r\"){\n  \n  \n  obje= \"reg:linear\"\n  metrics = list(eval_metric=\"rmse\",\n                 eval_metric=\"logloss\" \n  )\n  #best_model_parameter = \"rmse\"\n  optimizer =  \"which.min(rmseErrorsHyperparameters$rmse)\"\n  \n  if(type == \"c\"){\n    obje= \"binary:logistic\"\n    metrics = list(eval_metric=\"error\",\n                   #eval_metric=\"rmse\",\n                   eval_metric=\"auc\"\n    )\n    #best_model_parameter = \"auc\"\n    optimizer =  \"which.max(rmseErrorsHyperparameters$auc)\"\n  }\n  \n  \n  size_train = dim(data_train)[1]\n  \n  data_train[[target]]=NULL\n  data_test[[target]]=NULL\n  data = rbind(data_train, data_test)\n  dframe_treat = vtreat_vars(data)\n  \n  dframe_treat_train = dframe_treat[1:size_train,] #train\n  dframe_treat_test = dframe_treat[- c(1:size_train),] #test\n  \n  \n  #subset for now\n  #data.train.xgboost_1 = dtrain[sample.int(n = nrow(dtrain), size = floor(.05*nrow(dtrain)), replace = F), ]\n  \n  searchGridSubCol <- expand.grid(subsample = c(0.7, 1), \n                                  colsample_bytree = c(0.6, 0.8, 1),\n                                  eta = c(0.01, 0.001, 0.0001),\n                                  max_depth = c(100, 500) \n  )\n  \n  #subset for now\n  #dtrain_custsearch_subset = dtrain_custsearch[sample.int(n = nrow(dtrain_custsearch), size = floor(.01*nrow(dtrain_custsearch)), replace = F), ]\n  \n  \n  \n  #Build a xgb.DMatrix object\n  DMMatrixTrain <- xgb.DMatrix(data = as.matrix(dframe_treat_train), label = train_label)\n  DMMatrixTest <- xgb.DMatrix(data = as.matrix(dframe_treat_test), label = test_label)\n  \n  watchlist <- list(train=DMMatrixTrain, test=DMMatrixTest)\n  \n  \n  ErrorsHyperparameters <- apply(searchGridSubCol, 1, function(parameterList){\n    \n    #Extract Parameters to test\n    currentColsampleRate <- parameterList[[\"colsample_bytree\"]]\n    currentMax_depth <- parameterList[[\"max_depth\"]]\n    currentEta <- parameterList[[\"eta\"]]\n    currentSubsample <- parameterList[[\"subsample\"]]\n    \n    \n    param <- c(list(booster=\"gbtree\", \n                    objective=obje,\n                    #eval_metric=\"mlogloss\",\n                    #showsd = TRUE, \n                    #metrics=metrics\n                    \n                    \n                    #num_class=2,\n                    eta = currentEta,\n                    colsample_bytree = currentColsampleRate,\n                    #gamma = 1,\n                    max_depth = currentMax_depth, \n                    #min_child_weight = 1,\n                    subsample = currentSubsample\n                    #scale_pos_weight=scale_pos_weight),\n    ),\n    metrics\n    )\n    \n    \n    xgb2 <- xgb.train(data = DMMatrixTrain,\n                      params = param,\n                      watchlist=watchlist,\n                      nrounds = nrounds\n    )\n    \n    \n    if(type == \"c\"){\n      \n      test_auc = xgb2$evaluation_log[nrounds]$test_auc\n      test_error = xgb2$evaluation_log[nrounds]$test_error\n      return(c(test_error, test_auc, currentSubsample, currentColsampleRate, currentEta, currentMax_depth))\n      \n    }else if (type == \"r\"){\n      \n      test_rmse = xgb2$evaluation_log[nrounds]$test_rmse\n      test_logloss = xgb2$evaluation_log[nrounds]$test_logloss\n      return(c(test_rmse, test_logloss, currentSubsample, currentColsampleRate, currentEta, currentMax_depth))\n      \n    }\n    \n  })\n  \n  ErrorsHyperparameters=transpose(as.data.frame(ErrorsHyperparameters))\n  names(ErrorsHyperparameters)=c(unlist(metrics),names(searchGridSubCol))\n  best.parameters= ErrorsHyperparameters[  eval(parse(text = optimizer)) ,]\n  \n  fin_best_model=NULL\n  if (best_model==TRUE){\n    param <- c(list(booster=\"gbtree\", \n                    objective=obje,\n                    #num_class=2,\n                    eta = best.parameters$eta,\n                    colsample_bytree = best.parameters$colsample_bytree,\n                    #gamma = 1,\n                    max_depth = best.parameters$max_depth, \n                    #min_child_weight = 1,\n                    subsample = best.parameters$subsample\n    ), \n    metrics)\n    \n    \n    fin_best_model <- xgb.train(data = DMMatrixTrain,\n                                params = param,\n                                watchlist=watchlist,\n                                nrounds = nrounds\n    )\n    \n  }\n  \n  \n  return( list(best_parameters = best.parameters, result = ErrorsHyperparameters, fin_best_model = fin_best_model  ))\n  \n}",
    "created" : 1506364627552.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2582309877",
    "id" : "DFCE24A2",
    "lastKnownWriteTime" : 1506197997,
    "last_content_update" : 1506197997,
    "path" : "~/Documents/HU_Berlin/ML/DSG/DSG_2017_Finals/functions/xg_boost_baseline.R",
    "project_path" : "functions/xg_boost_baseline.R",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}